import streamlit as st
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
import os
import plotly.express as px
from datetime import datetime, timedelta
import time
import json
import re
from typing import Dict  # Python 3.8 compatibility

load_dotenv()

st.set_page_config(page_title="Survey LMS Dashboard", page_icon="ğŸ“Š", layout="wide")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SCHOOL NAME NORMALISATION ENGINE
#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  3-layer strategy:
#   1. Hard alias table  (built from every variant in your real SQL dump)
#   2. Zero-width / invisible-char stripping  (handles ZWSP embedded by phones)
#   3. "ğŸ« School Mapping" UI tab to add new aliases at runtime
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ALIAS_FILE = "school_aliases_custom.json"

# â”€â”€â”€ canonical school names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
C = {
    "chatumuk": "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á…áá»á˜á»á",
    "aranh": "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸",
    "preakleab": "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€á›áŸ€á”",
    "preakdambang": "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€áŠáŸ†á”á„",
    "preahangdong": "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„",
    "ksachsa": "áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸ",
    "sokhan_khvav": "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’áœá¶áœ",
    "sokhan_tonlap": "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“á‘á“áŸ’á›á¶á”áŸ‹",
    "sokhan_tramknar": "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš",
    "sokhan_preysandaek": "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“á–áŸ’ášáŸƒáŸááŸ’ááŸ‚á€",
    "sokhan_doungkhpos": "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“áŠá¼á„ááŸ’á–áŸáŸ‹á”á¼ášá¸á‡á›áŸá¶áš",
    "hunsaen_sereipheap": "áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“áŸáŸ‚á“áŸáŸášá¸á—á¶á–",
    "hunsaen_1mithuna": "áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“áŸáŸ‚á“áŸ¡á˜á·áá»á“á¶",
    "bunrany": "áœá·á‘áŸ’á™á¶á›áŸá™á”á»ááŸ’á™ášáŸ‰á¶á“á¸á áŸŠá»á“áŸáŸ‚á“á—áŸ’á“áŸ†á‡á¸áŸá¼áš",
    "haspveam": "áœá·á‘áŸ’á™á¶á›áŸá™á áŸ á–á¶á˜á‡á¸á€á„",
    "lve": "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á›áŸ’áœáŸ",
}

# â”€â”€â”€ alias table: every raw variant â†’ canonical â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ALL variants found in your live SQL dump are included.
SCHOOL_ALIASES_BUILTIN: Dict[str, str] = {
    # â•â• á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á…áá»á˜á»á â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "á£á“á»áœá·á‘áŸ’á™á¶á›áŸá™á…áá»á˜á»á": C["chatumuk"],  # á£ vs á¢
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á». á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á».á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á» á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»áœ.á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»áœá·.á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›á™áŸá…áá»á˜á»á": C["chatumuk"],  # á›á™áŸ typo
    "á¢á“á»áœá·á¡á¶á›áŸá™á…áá»á˜á»á": C["chatumuk"],  # áœá·á¡ typo
    "áŸá¶á›á¶ á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á…áá»á˜á»á": C["chatumuk"],
    "áŸá¶á›á¶á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á…áá»á˜á»á": C["chatumuk"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™â€‹á…áá»á˜á»á": C["chatumuk"],  # ZWSP after á›áŸá™
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™â€‹á…áá»á˜á»áâ€‹": C["chatumuk"],  # trailing ZWSP too
    # â•â• á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸": C["aranh"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸": C["aranh"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸": C["aranh"],  # á›áŸ typo
    "áŸá¶á›á¶á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸá¸": C["aranh"],
    "áŸá¶á›á¶á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á¢ášá‰áŸ’á‰ášá„áŸ’áŸáŸŠá¸": C["aranh"],  # áŸáŸŠá¸ variant
    "ARS": C["aranh"],  # abbreviation in DB
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    "áœá·.á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡ á¢á„áŸ’á‚ áŒá½á„": C["preahangdong"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡ á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá¼á„": C["preahangdong"],  # áŒá¼ typo
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„(NGS)": C["preahangdong"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„á€á˜áŸ’á˜áœá·á’á¸á‡áŸ†á“á¶á“áŸ‹ááŸ’á˜á¸": C["preahangdong"],
    "áŸá¶á›á¶ášáŸ€á“á‡áŸ†á“á¶á“áŸ‹ááŸ’á˜á¸á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    "áŸá¶á›á¶áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‡á¢á„áŸ’á‚áŒá½á„": C["preahangdong"],
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€á›áŸ€á” â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€á›áŸ€á”": C["preakleab"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ á–áŸ’ášáŸ‚á€á›áŸ€á”": C["preakleab"],
    "á€á˜áŸ’á˜áœá·á’á¸áŸá¶á›á¶ášáŸ€á“á‡áŸ†á“á¶á“áŸ‹ááŸ’á˜á¸ áœá·á‘áŸ’á™á¶á›áŸá™ á–áŸ’ášáŸ‚á€á›áŸ€á”": C["preakleab"],
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹á‡áŸ†á“á¶á“áŸ‹ááŸ’á˜á¸â€‹á–áŸ’ášáŸ‚á€á›áŸ€á”â€‹": C["preakleab"],  # ZWSP chars
    # â•â• á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€áŠáŸ†á”á„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "áŸá¶á›á¶á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "áŸá¶á›á¶á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™ á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "á¢á“á».á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "á¢á“á»á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],
    "á¢á“á»á–áŸ’ášáŸ‚á€ááŸ†á”á„": C["preakdambang"],  # ááŸ† typo
    "á¢á“á»á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],  # missing áœá·
    "á¢á“á»áœá·á‘áŸ’á™á¶á›á™áŸ á–áŸ’ášáŸ‚á€áŠáŸ†á”á„": C["preakdambang"],  # á›á™áŸ typo
    "á£á“á»áœá·á‘áŸ’á™á¶á›áŸá™á–áŸ’ášáŸá€áŠáŸ†á”á„": C["preakdambang"],  # á–áŸ’ášáŸ typo + á£
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸ": C["ksachsa"],
    "áœá·.ááŸ’áŸá¶á…áŸ‹áŸ": C["ksachsa"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ ááŸ’áŸá¶á…áŸ‹áŸ": C["ksachsa"],
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹ááŸ’áŸá¶á…áŸ‹áŸ": C["ksachsa"],  # ZWSP
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹ááŸ’áŸá¶á…áŸ‹â€‹áŸâ€‹": C["ksachsa"],  # multiple ZWSP
    "áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸâ€‹": C["ksachsa"],  # trailing ZWSP
    "áŸá¶á›á¶áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸ": C["ksachsa"],
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’áœá¶áœ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»á á¢á¶á“ ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»áá¢á¶á“ ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»áá¢á¶á“ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»á á¢á¶á“ááŸ’áœá¶áœ": C["sokhan_khvav"],
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹áŸá»áá¢á¶á“ááŸ’áœá¶áœ": C["sokhan_khvav"],  # ZWSP
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’áœá¶": C["sokhan_khvav"],  # missing áœ
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“á‘á“áŸ’á›á¶á”áŸ‹ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“á‘á“áŸ’á›á¶á”áŸ‹": C["sokhan_tonlap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»á á¢á¶á“ á‘á“áŸ’á›á¶á”áŸ‹": C["sokhan_tonlap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»áá¢á¶á“á‘á“áŸ’á›á¶á”áŸ‹": C["sokhan_tonlap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»áá¢á¶á“ á‘á“áŸ’á›á¶á”áŸ‹": C["sokhan_tonlap"],
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»áá¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš": C["sokhan_tramknar"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»áá¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš": C["sokhan_tramknar"],
    "áœá·á‘áŸ’á™á¶á›áŸá™áŸá»á á¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš": C["sokhan_tramknar"],
    "áœá·á‘áŸ’á™á¶á›á™áŸáŸá»áá¢á¶á“ááŸ’ášá¶áŸ†ááŸ’á“á¶áš": C["sokhan_tramknar"],  # á›á™áŸ typo
    # â•â• áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“áŸáŸ‚á“áŸáŸášá¸á—á¶á– â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“áŸáŸ‚á“áŸáŸášá¸á—á¶á–": C["hunsaen_sereipheap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ á áŸŠá»á“ áŸáŸ‚á“ áŸáŸášá¸á—á¶á–": C["hunsaen_sereipheap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ á áŸŠá»á“áŸáŸ‚á“áŸáŸášá¸á—á¶á–": C["hunsaen_sereipheap"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“ áŸáŸ‚á“áŸáŸášá¸á—á¶á–": C["hunsaen_sereipheap"],
    "áœá·á‘áŸ’á™á¶á›á™áŸá áŸŠá»á“áŸáŸ‚á“áŸáŸášá¸á—á¶á–": C["hunsaen_sereipheap"],  # á›á™áŸ typo
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹á áŸŠá»á“â€‹áŸáŸ‚á“â€‹áŸáŸášá¸á—á¶á–â€‹": C["hunsaen_sereipheap"],  # ZWSP
    "áŸá¶á›á¶áœá·á‘áŸ’á™á¶á›áŸá™â€‹ á áŸŠá»á“áŸáŸ‚á“â€‹ áŸáŸášá¸á—á¶á–â€‹": C["hunsaen_sereipheap"],
    # â•â• Single-entry schools â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "áœá·á‘áŸ’á™á¶á›áŸá™á áŸŠá»á“áŸáŸ‚á“áŸ¡á˜á·áá»á“á¶": C["hunsaen_1mithuna"],
    "áœá·á‘áŸ’á™á¶á›áŸá™á”áŸ‘á»á“ášáŸ‰á¶á“á¸á áŸ‘á»á“áŸáŸ‚á“á—áŸ’á“áŸ†á‡á¸áŸá¼áš": C["bunrany"],
    "áœá·á‘áŸ’á™á¶á›áŸá™â€‹ á áŸâ€‹ á–á¶á˜á‡á¸á€á„": C["haspveam"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»á á¢á¶á“á–áŸ’ášáŸƒáŸááŸ’ááŸ‚á€": C["sokhan_preysandaek"],
    "áœá·á‘áŸ’á™á¶á›áŸá™ áŸá»á á¢á¶á“ áŠá¼á„ááŸ’á–áŸáŸ‹á”á¼ášá¸á‡á›áŸá¶áš": C["sokhan_doungkhpos"],
    "á¢á“á»áœá·á‘áŸ’á™á¶á›áŸá™á›áŸ’áœáŸ": C["lve"],
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Normalisation helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_INVISIBLE = re.compile(
    r"[\u200b\u200c\u200d\u00ad\ufeff\u2060\u180e\u2028\u2029]",
    re.UNICODE,
)


def strip_invisible(text: str) -> str:
    """Remove zero-width and other invisible Unicode characters."""
    return _INVISIBLE.sub("", text)


def normalize_key(text: str) -> str:
    """
    Lookup key: strip invisible chars, collapse ALL whitespace to nothing,
    lower-case. This collapses space variants and ZWSP in one step.
    """
    if not isinstance(text, str):
        return str(text)
    return re.sub(r"\s+", "", strip_invisible(text)).lower()


def load_custom_aliases() -> dict:
    if os.path.exists(ALIAS_FILE):
        try:
            with open(ALIAS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_custom_aliases(aliases: dict):
    with open(ALIAS_FILE, "w", encoding="utf-8") as f:
        json.dump(aliases, f, ensure_ascii=False, indent=2)


def get_all_aliases() -> dict:
    merged = dict(SCHOOL_ALIASES_BUILTIN)
    merged.update(load_custom_aliases())
    return merged


def build_fast_lookup(alias_map: dict) -> dict:
    return {normalize_key(k): v for k, v in alias_map.items()}


def resolve_school(raw: str, fast_lookup: dict, alias_map: dict) -> str:
    if not isinstance(raw, str):
        return str(raw)
    # 1. exact match (fastest)
    if raw in alias_map:
        return alias_map[raw]
    # 2. space / ZWSP stripped match
    key = normalize_key(raw)
    if key in fast_lookup:
        return fast_lookup[key]
    # 3. fallback: at least strip invisible chars & tidy whitespace
    return re.sub(r"\s+", " ", strip_invisible(raw).strip())


def apply_normalization(df: pd.DataFrame) -> pd.DataFrame:
    alias_map = get_all_aliases()
    fast_lookup = build_fast_lookup(alias_map)
    df = df.copy()
    df["school_raw"] = df["school_name"]  # keep original for audit
    df["school_name"] = df["school_name"].apply(
        lambda x: resolve_school(x, fast_lookup, alias_map)
    )
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def get_connection_string():
    return (
        f"mysql+pymysql://{os.getenv('MYSQL_USER')}:{os.getenv('MYSQL_PASSWORD')}"
        f"@{os.getenv('MYSQL_HOST')}:{os.getenv('MYSQL_PORT')}"
        f"/{os.getenv('MYSQL_DATABASE')}?charset=utf8mb4"
    )


@st.cache_data(ttl=60)
def fetch_data():
    max_retries, retry_delay = 3, 2
    for attempt in range(max_retries):
        try:
            engine = create_engine(
                get_connection_string(),
                pool_pre_ping=True,
                pool_recycle=3600,
                connect_args={"connect_timeout": 10},
            )
            df = pd.read_sql(
                "SELECT * FROM survey_responses ORDER BY created_at DESC", engine
            )
            engine.dispose()
            df = apply_normalization(df)  # â† normalise immediately after fetch
            return df
        except Exception as e:
            if attempt < max_retries - 1:
                st.warning(f"Retryingâ€¦ ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
            else:
                st.error(f"âŒ Failed to connect: {e}")
                st.info("Check if MySQL is running: `docker ps`")
                return pd.DataFrame()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    st.title("ğŸ“Š Survey LMS Dashboard")
    st.markdown("---")

    col1, col2, _ = st.columns([1, 1, 4])
    with col1:
        if st.button("ğŸ”„ Refresh Data"):
            st.cache_data.clear()
            st.rerun()
    with col2:
        try:
            from sqlalchemy import text

            engine = create_engine(
                get_connection_string(), connect_args={"connect_timeout": 10}
            )
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            st.success("âœ… Connected")
            engine.dispose()
        except Exception as e:
            st.error(f"âŒ DB Offline: {e}")
            st.stop()

    try:
        df = fetch_data()
        if df.empty:
            st.warning("No survey responses yet.")
            return

        # â”€â”€ SIDEBAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.sidebar.header("ğŸ” Filters")

        # School dropdown uses canonical names â†’ no duplicates
        schools = ["All"] + sorted(df["school_name"].unique().tolist())
        selected_school = st.sidebar.selectbox("School Name", schools)
        classes = ["All"] + sorted(df["class_name"].unique().tolist())
        selected_class = st.sidebar.selectbox("Class", classes)
        computer_options = ["All", "á€. á’áŸ’á›á¶á”áŸ‹", "á. á˜á·á“á’áŸ’á›á¶á”áŸ‹"]
        selected_computer = st.sidebar.selectbox(
            "Computer Experience", computer_options
        )

        st.sidebar.subheader("Date Range")
        date_option = st.sidebar.radio(
            "Select period:",
            ["All time", "Last 7 days", "Last 30 days", "Custom range"],
        )

        # â”€â”€ APPLY FILTERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        filtered_df = df.copy()
        if selected_school != "All":
            filtered_df = filtered_df[filtered_df["school_name"] == selected_school]
        if selected_class != "All":
            filtered_df = filtered_df[filtered_df["class_name"] == selected_class]
        if selected_computer != "All":
            filtered_df = filtered_df[
                filtered_df["computer_usage"] == selected_computer
            ]

        if date_option == "Last 7 days":
            filtered_df = filtered_df[
                filtered_df["created_at"] >= datetime.now() - timedelta(days=7)
            ]
        elif date_option == "Last 30 days":
            filtered_df = filtered_df[
                filtered_df["created_at"] >= datetime.now() - timedelta(days=30)
            ]
        elif date_option == "Custom range":
            c1, c2 = st.sidebar.columns(2)
            start_date = c1.date_input("From")
            end_date = c2.date_input("To")
            filtered_df = filtered_df[
                (filtered_df["created_at"].dt.date >= start_date)
                & (filtered_df["created_at"].dt.date <= end_date)
            ]

        # â”€â”€ KEY METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.header("ğŸ“ˆ Key Metrics")
        m1, m2, m3, m4 = st.columns(4)
        with m1:
            st.metric("Total Responses", len(filtered_df))
        with m2:
            no_computer = len(filtered_df[filtered_df["computer_usage"] == "á. á˜á·á“á’áŸ’á›á¶á”áŸ‹"])
            st.metric("No Computer Experience", no_computer)
        with m3:
            st.metric("Schools (unique)", filtered_df["school_name"].nunique())
        with m4:
            if not filtered_df.empty:
                latest = filtered_df["created_at"].max()
                hours_ago = int((datetime.now() - latest).total_seconds() / 3600)
                st.metric("Latest Response", f"{hours_ago}h ago")

        st.markdown("---")

        # â”€â”€ TABS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tab1, tab2, tab3, tab4, tab5 = st.tabs(
            [
                "ğŸ“Š Overview",
                "â“ Questions",
                "ğŸ“… Timeline",
                "ğŸ” Custom Queries",
                "ğŸ« School Mapping",
            ]
        )

        # TAB 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab1:
            col1, col2 = st.columns(2)
            with col1:
                sc = filtered_df["school_name"].value_counts().reset_index()
                sc.columns = ["School", "Responses"]
                fig = px.bar(
                    sc,
                    x="School",
                    y="Responses",
                    title="Responses by School (duplicates merged)",
                    color="Responses",
                    color_continuous_scale="Blues",
                )
                fig.update_layout(xaxis_tickangle=-30)
                st.plotly_chart(fig, use_container_width=True)
            with col2:
                cc = filtered_df["computer_usage"].value_counts().reset_index()
                cc.columns = ["Experience", "Count"]
                st.plotly_chart(
                    px.pie(
                        cc,
                        names="Experience",
                        values="Count",
                        title="Computer Usage Distribution",
                    ),
                    use_container_width=True,
                )

        # TAB 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab2:
            answered = filtered_df[filtered_df["question_1"] != "N/A"]
            if len(answered) == 0:
                st.info("No completed responses in current filter.")
            else:
                col1, col2 = st.columns(2)
                with col1:
                    q1 = answered["question_1"].value_counts().reset_index()
                    q1.columns = ["Answer", "Count"]
                    st.plotly_chart(
                        px.bar(q1, x="Answer", y="Count", title="Q1: Study Hours"),
                        use_container_width=True,
                    )
                with col2:
                    q2 = answered["question_2"].value_counts().reset_index()
                    q2.columns = ["Answer", "Count"]
                    st.plotly_chart(
                        px.pie(
                            q2,
                            names="Answer",
                            values="Count",
                            title="Q2: Learning Method",
                        ),
                        use_container_width=True,
                    )

        # TAB 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab3:
            tl = filtered_df.copy()
            tl["date"] = tl["created_at"].dt.date
            daily = tl.groupby("date").size().reset_index()
            daily.columns = ["Date", "Responses"]
            st.plotly_chart(
                px.line(
                    daily,
                    x="Date",
                    y="Responses",
                    title="Daily Responses",
                    markers=True,
                ),
                use_container_width=True,
            )

        # TAB 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab4:
            st.subheader("Teachers Without Computer Experience")
            no_comp = filtered_df[filtered_df["computer_usage"] == "á. á˜á·á“á’áŸ’á›á¶á”áŸ‹"]
            st.metric("Count", len(no_comp))
            if len(no_comp) > 0:
                st.dataframe(
                    no_comp[["full_name", "school_name", "class_name", "created_at"]],
                    use_container_width=True,
                )

            st.markdown("---")
            st.subheader("School vs Computer Usage Cross-Tab")
            st.dataframe(
                pd.crosstab(filtered_df["school_name"], filtered_df["computer_usage"]),
                use_container_width=True,
            )

            st.markdown("---")
            st.subheader("ğŸ”€ Name Variants Merged")
            st.caption("Every raw name that was mapped to a canonical name.")
            if "school_raw" in filtered_df.columns:
                vdf = (
                    filtered_df[
                        filtered_df["school_raw"] != filtered_df["school_name"]
                    ][["school_raw", "school_name"]]
                    .drop_duplicates()
                    .rename(
                        columns={
                            "school_raw": "As Written by Teacher",
                            "school_name": "Canonical Name",
                        }
                    )
                    .sort_values("Canonical Name")
                )
                if vdf.empty:
                    st.success("âœ… No variants in current filter.")
                else:
                    st.dataframe(vdf, use_container_width=True)

            st.markdown("---")
            st.subheader("âš ï¸ Unresolved School Names")
            st.caption("Names not in any alias table â€” go to ğŸ« School Mapping to fix.")
            all_canonical = set(get_all_aliases().values())
            unresolved = (
                filtered_df[~filtered_df["school_name"].isin(all_canonical)][
                    "school_name"
                ]
                .value_counts()
                .reset_index()
            )
            unresolved.columns = ["School Name", "Count"]
            if unresolved.empty:
                st.success("âœ… All school names resolved.")
            else:
                st.warning(
                    f"{len(unresolved)} unresolved name(s). Add them in ğŸ« School Mapping."
                )
                st.dataframe(unresolved, use_container_width=True)

        # TAB 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab5:
            st.subheader("ğŸ« School Name Alias Manager")
            st.info(
                "When a teacher writes a school name in a new way, add it here.  \n"
                "Changes take effect after **ğŸ”„ Refresh Data**."
            )

            custom_aliases = load_custom_aliases()

            with st.form("add_alias_form", clear_on_submit=True):
                st.markdown("**â• Add New Alias**")
                c1, c2 = st.columns(2)
                new_raw = c1.text_input(
                    "Alias (as typed by teacher)", placeholder="e.g. áœá·.ááŸ’áŸá¶á…áŸ‹ áŸ"
                )
                new_canonical = c2.text_input(
                    "Canonical (official) name", placeholder="e.g. áœá·á‘áŸ’á™á¶á›áŸá™ááŸ’áŸá¶á…áŸ‹áŸ"
                )
                if st.form_submit_button("âœ… Save Alias"):
                    if new_raw.strip() and new_canonical.strip():
                        custom_aliases[new_raw.strip()] = new_canonical.strip()
                        save_custom_aliases(custom_aliases)
                        st.cache_data.clear()
                        st.success(f"Saved: **{new_raw}** â†’ **{new_canonical}**")
                        st.rerun()
                    else:
                        st.warning("Both fields are required.")

            st.markdown("---")
            st.markdown("**ğŸ“‹ Your Custom Aliases**")
            if custom_aliases:
                ca_df = pd.DataFrame(
                    list(custom_aliases.items()),
                    columns=["Alias (Raw)", "Canonical Name"],
                )
                st.dataframe(ca_df, use_container_width=True)
                del_key = st.selectbox(
                    "Select alias to delete",
                    ["â€” none â€”"] + list(custom_aliases.keys()),
                )
                if st.button("ğŸ—‘ï¸ Delete") and del_key != "â€” none â€”":
                    del custom_aliases[del_key]
                    save_custom_aliases(custom_aliases)
                    st.cache_data.clear()
                    st.success(f"Deleted: {del_key}")
                    st.rerun()
            else:
                st.caption("No custom aliases yet.")

            st.markdown("---")
            st.markdown("**ğŸ“š Built-in Alias Table (from code)**")
            builtin_df = pd.DataFrame(
                sorted(SCHOOL_ALIASES_BUILTIN.items(), key=lambda x: x[1]),
                columns=["Alias (Raw)", "Canonical Name"],
            )
            st.dataframe(builtin_df, use_container_width=True)

        # â”€â”€ DATA TABLE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        st.header("ğŸ“‹ All Responses")

        search = st.text_input("ğŸ” Search by name, school, or class")
        if search:
            display_df = filtered_df[
                filtered_df["full_name"]
                .astype(str)
                .str.contains(search, case=False, na=False)
                | filtered_df["school_name"]
                .astype(str)
                .str.contains(search, case=False, na=False)
                | filtered_df["school_raw"]
                .astype(str)
                .str.contains(search, case=False, na=False)
                | filtered_df["class_name"]
                .astype(str)
                .str.contains(search, case=False, na=False)
            ]
        else:
            display_df = filtered_df

        st.dataframe(display_df, use_container_width=True, height=400)

        # â”€â”€ EXPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        col_dl, _ = st.columns([1, 3])
        with col_dl:
            csv = display_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="ğŸ“¥ Download CSV",
                data=csv,
                file_name=f"survey_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
            )

    except Exception as e:
        st.error(f"Error: {e}")
        st.exception(e)


if __name__ == "__main__":
    main()
